# Large Language Model Code Translation Research

## Overview
This repository contains the research project on the effectiveness of using large language models (LLMs) for program translation. The focus is on the readability, similarity to human-generated reference solutions, and syntactical correctness of the translated code.

## Project Structure
The repository is structured as follows:
- `data/`: Contains the original and translated code snippets used for testing.
- `intermediate-rep/`: Holds the intermediate representations (pseudocode and descriptions) generated by the LLM.
- `translated-code/`: Stores the code translated directly from one language to Python and from intermediate representations to Python.
- `scripts/`: Contains all the scripts used for generating translations, evaluating them, and organizing the data.
- `results/`: Location where the output of evaluation metrics (like BLEU scores, syntax correctness, etc.) are stored.
- `visualizations/`: Stores visual representations of the data such as graphs and charts.

## Installation and Setup
Describe how to set up the environment to run the scripts, including installing any dependencies, setting up virtual environments, etc.

```bash
# Example for setting up a virtual environment and installing dependencies
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Usage
First navigate to the `scripts` directory. Then to evaluate the generated solutions: 

```python3 evaluate.py```
